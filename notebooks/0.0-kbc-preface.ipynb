{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning for Physical Interaction through Video Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Authors: C. Finn, I. Goodfellow, S. Levine.\n",
    "- Year: 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on an initial state (i.e: the position of the robot) and an action to execute (i.e: push a block from x1, y1 to x2, y2), predict the physiscal interaction before executing the action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models learn physic instead of the object appearance: makes them able to generalize unseen object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three models are proposed in this paper:\n",
    "    1. Dynamic Neural Advection (DNA)\n",
    "        - For pixels that are constrained in a local region;\n",
    "        - Outputs a distrbution over locations in the previoys frame for each pixel in the new frame;\n",
    "        - the predicted pixel value becomes the expectation under the distribution.\n",
    "    2. Convolutional Dynamix Neural Advection (CDNA)\n",
    "        - Variant of a DNA;\n",
    "        - Output multupe normalized convolution kernels to apply to the previous image to compute new pixel values.\n",
    "    3. Spatial Transformer Predictors (STP)\n",
    "        - Output the parameters of multiple affine transformations to apply to the previous image\n",
    "        - The predicted transformation handle separate objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core trunk of each models is made of:\n",
    "    1. One 5x5 convolytion with a stride of 2;\n",
    "    2. Seven convolutional LSTMS;\n",
    "    3. A full-resolution mask for compositing the various transformed predictions (CDNA and STP only);\n",
    "    4. **** Two skip connections exists in the network to preserve high-resolution informations: \n",
    "        - LSTM 1 to convolution 2;\n",
    "        - LSTM 3 to LSTM 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between the three models are:\n",
    "    1. CDNA:\n",
    "        - Ten filters of size 5 x 5 are created in the transormation area, they're normalized to via a spatial softmax;\n",
    "        - **** The spatial is used to return the expected pixiel location of the new image based on the distribution of the filters over the previous image\n",
    "        - The transformations corresponds to a convolution\n",
    "    2. STP:\n",
    "        - Ten filters of size 3 x 2 of affine transformation matrices (with a [spatial transformer](https://arxiv.org/abs/1506.02025))\n",
    "        - The transformations are applied to the preceding image to create 10 separate transformed images\n",
    "        - The transformations correspond to an affine transformation\n",
    "    3. DNA:\n",
    "        - No filters for the transformation\n",
    "        - The transformation parameters are outputted at the last layer, in the same place as the mask\n",
    "        - The transformation correspond to a 5 x 5 convolutional kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-deterministic behaviour in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some operation, TensorFlow is non-deterministic (the same result is not guaranteed at each iteration) for both the GPU and the CPU. Especialy, in our case, the function \"reduce_sum\" may have a different behaviour at each executions. To test that, the code bellow was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def debug(tensor, session, feed_dict):\n",
    "    arr = tensor.eval(session=session, feed_dict=feed_dict)\n",
    "    return arr[0][0][0][0][0] # Print the first element\n",
    "    \n",
    "\n",
    "stop = 1500\n",
    "i = 0\n",
    "while i < stop:\n",
    "\tnorm_factor = tf.reduce_sum(cdna_kerns, [1, 2, 3], keep_dims=True)\n",
    "\tDebug.push(norm_factor, \"Norm factor 1\", deb)\n",
    "\ti += 1\n",
    "    \n",
    "# cdna_kerns is an array of shape (32, 5, 5, 1, 10)\n",
    "# The average result at arr[0][0][0][0][0] is \"\", min is \"\" and max is \"\"\n",
    "# The result is supposed to be 19.780447"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow to Chainer and Numpy conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow are the conversion for TensorFlow's functions to Chainer's and Numpy's functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python import layers as tf_layers\n",
    "import chainer as chainer\n",
    "\n",
    "def print_tf_shape(tensor):\n",
    "    print(\"[TF] Shape is {}\".format(tensor.get_shape()))\n",
    "\n",
    "def print_ch_shape(variable):\n",
    "    print(\"[Chainer] Shape is {}\".format(variable.shape))\n",
    "\n",
    "def print_np_shape(array):\n",
    "    print(\"[Numpy] Shape is {}\".format(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[TF] Shape is (9,)\n",
      "[Chainer] Shape is (9,)\n"
     ]
    }
   ],
   "source": [
    "# Create a Tensor/Variable\n",
    "x = np.arange(9.0)\n",
    "tf_res = tf.constant(x)\n",
    "ch_res = chainer.variable.Variable(x)\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[TF] Shape is (2, 3)\n",
      "[TF] Shape is (2, 3)\n",
      "2\n",
      "[Chainer] Shape is (2, 3)\n",
      "[Chainer] Shape is (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split an array into multiple sub-arrays\n",
    "x = np.random.randint(0, 255,(2,6))\n",
    "tf_res = tf.split(axis=1, num_or_size_splits=2, value=x)\n",
    "ch_res = chainer.functions.split_axis(chainer.variable.Variable(x), indices_or_sections=2, axis=1)\n",
    "print(len(tf_res))\n",
    "print_tf_shape(tf_res[0])\n",
    "print_tf_shape(tf_res[1])\n",
    "print(len(ch_res))\n",
    "print_ch_shape(ch_res[0])\n",
    "print_ch_shape(ch_res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (64, 32, 32, 32)\n",
      "[Chainer] Shape is (64, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Join a sequence of arrays along an existing axis\n",
    "x = np.random.randint(0,255, (32, 32, 32, 32))\n",
    "y = np.random.randint(0,255, (32, 32, 32, 32))\n",
    "tf_res = tf.concat(axis=0, values=[tf.constant(x), tf.constant(y)])\n",
    "ch_res = chainer.functions.concat((chainer.variable.Variable(x), chainer.variable.Variable(y)), axis=0)\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 32768)\n",
      "[Chainer] Shape is (32, 32768)\n"
     ]
    }
   ],
   "source": [
    "# Gives a new shape to an array without changing its data\n",
    "x = np.random.randint(0.,255., (32, 32, 32, 32))\n",
    "tf_res = tf.reshape(tf.constant(x), [x.shape[0], -1])\n",
    "ch_res = chainer.functions.reshape(chainer.variable.Variable(x), (x.shape[0], -1))\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (2, 2, 2, 2)\n",
      "[Chainer] Shape is (2, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Construct an array by repeating the number of times given by reps\n",
    "x = np.random.randint(0.,255., (1, 1, 1, 1))\n",
    "tf_res = tf.tile(x, [2,2,2,2])\n",
    "ch_res = chainer.functions.tile(x, (2,2,2,2))\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No variables to optimize.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a4b4496c519d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtf_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mch_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet50Layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kristof/.pyenv/versions/2.7.11/envs/unsupervised_learning_physical_interaction_video_prediction/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kristof/.pyenv/versions/2.7.11/envs/unsupervised_learning_physical_interaction_video_prediction/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0mprocessors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No variables to optimize.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0mvar_refs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     grads = gradients.gradients(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to optimize."
     ]
    }
   ],
   "source": [
    "# AdamOptimizer\n",
    "from chainer.links.model.vision import resnet\n",
    "learning_rate = 0.01\n",
    "tf_res = tf.train.AdamOptimizer(learning_rate).minimize(tf.constant([]))\n",
    "ch_res = chainer.optimizers.Adam(alpha=learning_rate)\n",
    "model = resnet.ResNet50Layers()\n",
    "ch_res.setup(model)\n",
    "# ...\n",
    "ch_res.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 32, 32, 32)\n",
      "[Chainer] Shape is (32, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# 2D convolution\n",
    "tf_image = np.float32(np.random.randint(0.,255., (32, 64, 64, 3)))\n",
    "chainer_image = np.float32(np.random.randint(0.,255., (32, 3, 64, 64)))\n",
    "tf_res = tf.contrib.slim.layers.conv2d(tf_image, 32, [5, 5], stride=2, normalizer_fn=None)\n",
    "ch_res = chainer.links.Convolution2D(in_channels=3, out_channels=32, ksize=(5, 5), stride=2, pad=5/2)(chainer_image) \n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 64, 64, 3)\n",
      "[Chainer] Shape is (32, 3, 64, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristof/.pyenv/versions/2.7.11/envs/unsupervised_learning_physical_interaction_video_prediction/lib/python2.7/site-packages/chainer/utils/experimental.py:104: FutureWarning: chainer.links.normalization.layer_normalization.py is experimental. The interface can change in the future.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Layer normalization\n",
    "tf_image = np.float32(np.random.randint(0.,255., (32, 64, 64, 3)))\n",
    "chainer_image = np.float32(np.random.randint(0.,255., (32, 3, 64, 64)))\n",
    "tf_res = tf_layers.layer_norm(tf_image)\n",
    "\n",
    "ch_res = chainer.functions.reshape(chainer_image, (chainer_image.shape[0], -1))\n",
    "ch_res = chainer.links.LayerNormalization()(ch_res)\n",
    "ch_res = chainer.functions.reshape(ch_res, (chainer_image.shape[0], \n",
    "                                            chainer_image.shape[1], \n",
    "                                            chainer_image.shape[2], \n",
    "                                            chainer_image.shape[3]))\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 128, 128, 3)\n",
      "[Chainer] Shape is (32, 3, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# 2D Deconvolution\n",
    "tf_image = np.float32(np.random.randint(0.,255., (32, 64, 64, 3)))\n",
    "chainer_image = np.float32(np.random.randint(0.,255., (32, 3, 64, 64)))\n",
    "tf_res = tf.contrib.slim.layers.conv2d_transpose(tf.constant(tf_image), tf_image.shape[3], 3, stride=2)\n",
    "ch_res = chainer.links.Deconvolution2D(in_channels=chainer_image.shape[1], \n",
    "                                       out_channels=chainer_image.shape[1], \n",
    "                                       ksize=(3,3), \n",
    "                                       stride=2, \n",
    "                                       outsize=(chainer_image.shape[2]*2, chainer_image.shape[3]*2), pad=3/2)(\n",
    "                                            chainer.variable.Variable(chainer_image)\n",
    "                                       )\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 64, 64, 3)\n",
      "[Chainer] Shape is (32, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "tf_image = np.float32(np.random.randint(0.,255., (32, 64, 64, 3)))\n",
    "chainer_image = np.float32(np.random.randint(0.,255., (32, 3, 64, 64)))\n",
    "tf_res = tf.nn.softmax(tf.constant(tf_image))\n",
    "ch_res = chainer.functions.softmax(chainer.variable.Variable(chainer_image))\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF] Shape is (32, 64, 64, 3)\n",
      "[Chainer] Shape is (32, 3, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "# Relu\n",
    "tf_image = np.float32(np.random.randint(0.,255., (32, 64, 64, 3)))\n",
    "chainer_image = np.float32(np.random.randint(0.,255., (32, 3, 64, 64)))\n",
    "tf_res = tf.nn.relu(tf.constant(tf_image))\n",
    "ch_res = chainer.functions.relu(chainer_image)\n",
    "print_tf_shape(tf_res)\n",
    "print_ch_shape(ch_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
