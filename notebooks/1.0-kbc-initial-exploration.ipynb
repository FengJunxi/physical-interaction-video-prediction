{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step required to conduct the experiments was to convert Finn's architecture to Chainer's code. There's no automatic processes. To accomplish this task, a manual conversion was required. Below is some snippets of code illutrating the differences between TF code and Chainer's code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used to import the dataset is praticaly the same, except that the actions, states and images files are extracted from the original TF binary file and store in the `/data/processed` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_ref = []\n",
    "for j in xrange(len(files)):\n",
    "    logger.info(\"Creating data from tsrecords {0}/{1}\".format(j+1, len(files)))\n",
    "    raw, act, sta = sess.run([image_seq, action_seq, state_seq])\n",
    "    ref = []\n",
    "    ref.append(j)\n",
    "\n",
    "    if create_img == 1:\n",
    "        for k in xrange(raw.shape[0]):\n",
    "            img = Image.fromarray(raw[k], 'RGB')\n",
    "            img.save(out_dir + '/image_batch_' + str(j) + '_' + str(k) + '.png')\n",
    "        ref.append('image_batch_' + str(j) + '_*' + '.png')\n",
    "    else:\n",
    "        ref.append('')\n",
    "\n",
    "    np.save(out_dir + '/image_batch_' + str(j), raw)\n",
    "    np.save(out_dir + '/action_batch_' + str(j), act)\n",
    "    np.save(out_dir + '/state_batch_' + str(j), sta)\n",
    "\n",
    "    ref.append('image_batch_' + str(j) + '.npy')\n",
    "    ref.append('action_batch_' + str(j) + '.npy')\n",
    "    ref.append('state_batch_' + str(j) + '.npy')\n",
    "    csv_ref.append(ref)\n",
    "\n",
    "logger.info(\"Writing the results into map file '{0}'\".format('map.csv'))\n",
    "with open(out_dir + '/map.csv', 'wb') as csvfile:\n",
    "    writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(['id', 'img_bitmap_path', 'img_np_path', 'action_np_path', 'state_np_path'])\n",
    "    for row in csv_ref:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, this part of the importation need to be created only once since the files need to be extracted only one time. After that, it's necessary to import the actions, states and images in the code. Conveniently, they can be easily importer following the CSV map file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logger.info(\"Fetching the models and inputs\")\n",
    "data_map = []\n",
    "with open(data_dir + '/map.csv', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        data_map.append(row)\n",
    "\n",
    "if len(data_map) <= 1: # empty or only header\n",
    "    logger.error(\"No file map found\")\n",
    "    exit()\n",
    "\n",
    "# Load the images, actions and states\n",
    "images = []\n",
    "actions = []\n",
    "states = []\n",
    "for i in xrange(1, len(data_map)): # Exclude the header\n",
    "    logger.info(\"Loading data {0}/{1}\".format(i, len(data_map)-1))\n",
    "    images.append(np.load(data_dir + '/' + data_map[i][2]))\n",
    "    actions.append(np.load(data_dir + '/' + data_map[i][3]))\n",
    "    states.append(np.load(data_dir + '/' + data_map[i][4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to understand how TensorFlow works. TF create a computation graph first then runs it multiple time depedending of the number of iteration to realize. Therefore, one main difference between the original code and Chaine's code is how the model is created and modified between each operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow.\n",
    "# The model is created through a 'scope' meaning that the variable used in the creation of this 'model' can only be \n",
    "# seen by this model.\n",
    "with tf.variable_scope('model', reuse=None) as training_scope:\n",
    "  images, actions, states = build_tfrecord_input(training=True)\n",
    "  model = Model(images, actions, states, FLAGS.sequence_length,\n",
    "                prefix='train')\n",
    "\n",
    "with tf.variable_scope('val_model', reuse=None):\n",
    "  val_images, val_actions, val_states = build_tfrecord_input(training=False)\n",
    "  val_model = Model(val_images, val_actions, val_states,\n",
    "                    FLAGS.sequence_length, training_scope, prefix='val')\n",
    "    \n",
    "# ...\n",
    "\n",
    "# After building the computation graph, the model is trained via this training loop.\n",
    "for itr in range(FLAGS.num_iterations):\n",
    "  # feed_dict is a dictionary containing the placeholder (the variable replaced in each iteration of the loop)\n",
    "  # and the values.\n",
    "  # E.g: in the scope 'model' iter_num is replaced by the value of 'np.float32(itr)'\n",
    "  feed_dict = {model.iter_num: np.float32(itr),\n",
    "               model.lr: FLAGS.learning_rate}\n",
    "    \n",
    "  # Finally, to execute one epoch, 'sess.run' is called\n",
    "  cost, _, summary_str = sess.run([model.loss, model.train_op, model.summ_op],\n",
    "                                  feed_dict)\n",
    "    \n",
    "# Chainer.\n",
    "# Before, in TF, images, actions, states, ... were passed directly to the model.\n",
    "# Here, only the prefix and the model type is used\n",
    "training_model = Model(\n",
    "    is_cdna=model_type == 'CDNA',\n",
    "    is_dna=model_type == 'DNA',\n",
    "    is_stp=model_type == 'STP',\n",
    "    prefix='train'\n",
    ")\n",
    "validation_model = Model(\n",
    "    is_cdna=model_type == 'CDNA',\n",
    "    is_dna=model_type == 'DNA',\n",
    "    is_stp=model_type == 'STP',\n",
    "    prefix='val'\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "for itr in xrange(epoch):\n",
    "    # ...\n",
    "    \n",
    "    # Instead of using placeholders, we use the variable directly in Chainer\n",
    "    # 'img_training_set', 'act_training_set' and 'sta_training_set' are random mini-batches of data \n",
    "    # fed in each iteration\n",
    "    loss, psnr_all, summaries = training_model(\n",
    "        img_training_set, \n",
    "        act_training_set, \n",
    "        sta_training_set, \n",
    "        itr, \n",
    "        schedsamp_k, \n",
    "        use_state, \n",
    "        num_masks, \n",
    "        context_frames\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first iteration, the main goal was to reproduce as exactly as possible TF code in Chainer. Thus, the same 'stateless' architecture found in TF was used in Chainer. Below is an abstract of some of the key differences. The code can be found up to commit `2346519b30f045181985e9d9dbceb0dc57214fa0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stateless LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Stateless' because the instance of the LSTM class is recreated at each iteration: reseting its internal state. To be able to continue the computation work betweem each iteration, the previous 'state', which contain the `cell state` and the `cell hidden state`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "def basic_conv_lstm_cell(inputs, state, num_channels, filter_size=5, forget_bias=1.0, scope=None, reuse=None):\n",
    "    c, h = tf.split(axis=3, num_or_size_splits=2, value=state)\n",
    "    inputs_h = tf.concat(axis=3, values=[inputs, h])\n",
    "\n",
    "    i_j_f_o = layers.conv2d(inputs_h,\n",
    "                            4 * num_channels, [filter_size, filter_size],\n",
    "                            stride=1,\n",
    "                            activation_fn=None,\n",
    "                            scope='Gates')\n",
    "\n",
    "    i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=i_j_f_o)\n",
    "\n",
    "    new_c = c * tf.sigmoid(f + forget_bias) + tf.sigmoid(i) * tf.tanh(j)\n",
    "    new_h = tf.tanh(new_c) * tf.sigmoid(o)\n",
    "\n",
    "return new_h, tf.concat(axis=3, values=[new_c, new_h])\n",
    "\n",
    "# Chainer\n",
    "def __call__(self, inputs, state, num_channels, filter_size=5, forget_bias=1.0):\n",
    "    h, c = F.split_axis(state, indices_or_sections=2, axis=1)\n",
    "    inputs_h = F.concat((inputs, h))\n",
    "\n",
    "    i_j_f_o = L.Convolution2D(\n",
    "        in_channels=inputs_h.shape[1], \n",
    "        out_channels=4*num_channels, \n",
    "        ksize=(filter_size, filter_size), \n",
    "        pad=filter_size/2\n",
    "    )(inputs_h)\n",
    "\n",
    "    i, j, f, o = F.split_axis(i_j_f_o, indices_or_sections=4, axis=1)\n",
    "\n",
    "    new_c = c * F.sigmoid(f + forget_bias) + F.sigmoid(i) * F.tanh(j)\n",
    "    new_h = F.tanh(new_c) * F.sigmoid(o)\n",
    "\n",
    "    return new_h, F.concat((new_c, new_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StatelessModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is a wrapper around all the other models: StatelessCDNA, StatelessDNA and StatelessSTP. It's used to compute the loss and other common statistics between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "class Model(object):\n",
    "  def __init__(self, images=None, actions=None, states=None, sequence_length=None, reuse_scope=None, prefix=None):\n",
    "    #...\n",
    "    \n",
    "    gen_images, gen_states = construct_model(\n",
    "        images,\n",
    "        actions,\n",
    "        states,\n",
    "        iter_num=self.iter_num,\n",
    "        k=FLAGS.schedsamp_k,\n",
    "        use_state=FLAGS.use_state,\n",
    "        num_masks=FLAGS.num_masks,\n",
    "        cdna=FLAGS.model == 'CDNA',\n",
    "        dna=FLAGS.model == 'DNA',\n",
    "        stp=FLAGS.model == 'STP',\n",
    "        context_frames=FLAGS.context_frames\n",
    "    )\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # L2 loss, PSNR for eval.\n",
    "    loss, psnr_all = 0.0, 0.0\n",
    "    for i, x, gx in zip(\n",
    "        range(len(gen_images)), images[FLAGS.context_frames:],\n",
    "        gen_images[FLAGS.context_frames - 1:]):\n",
    "      recon_cost = mean_squared_error(x, gx)\n",
    "      psnr_i = peak_signal_to_noise_ratio(x, gx)\n",
    "      psnr_all += psnr_i\n",
    "      summaries.append(\n",
    "          tf.summary.scalar(prefix + '_recon_cost' + str(i), recon_cost))\n",
    "      summaries.append(tf.summary.scalar(prefix + '_psnr' + str(i), psnr_i))\n",
    "      loss += recon_cost\n",
    "\n",
    "    for i, state, gen_state in zip(\n",
    "        range(len(gen_states)), states[FLAGS.context_frames:],\n",
    "        gen_states[FLAGS.context_frames - 1:]):\n",
    "      state_cost = mean_squared_error(state, gen_state) * 1e-4\n",
    "      summaries.append(\n",
    "          tf.summary.scalar(prefix + '_state_cost' + str(i), state_cost))\n",
    "      loss += state_cost\n",
    "    summaries.append(tf.summary.scalar(prefix + '_psnr_all', psnr_all))\n",
    "    self.psnr_all = psnr_all\n",
    "\n",
    "    self.loss = loss = loss / np.float32(len(images) - FLAGS.context_frames)\n",
    "\n",
    "    summaries.append(tf.summary.scalar(prefix + '_loss', loss))\n",
    "\n",
    "    self.lr = tf.placeholder_with_default(FLAGS.learning_rate, ())\n",
    "\n",
    "    self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "    self.summ_op = tf.summary.merge(summaries)\n",
    "\n",
    "# Chainer\n",
    "class Model(chainer.Chain):\n",
    "    def __init__(self, is_cdna=True, is_dna=False, is_stp=False, prefix=None):\n",
    "        # ...\n",
    "    def __call__(self, images, actions=None, states=None, iter_num=-1.0, scheduled_sampling_k=-1, \n",
    "                 use_state=True, num_masks=10, num_frame_before_prediction=2):\n",
    "        gen_images, gen_states = self.model(images, actions, states, iter_num, scheduled_sampling_k, \n",
    "                                            use_state, num_masks, fore_prediction)\n",
    "\n",
    "    # L2 loss, PSNR for eval\n",
    "    loss, psnr_all = 0.0, 0.0\n",
    "    summaries = []\n",
    "    for i, x, gx in zip(range(len(gen_images)), images[num_frame_before_prediction:], gen_images[num_frame_before_prediction - 1:]):\n",
    "        x = variable.Variable(x)\n",
    "        recon_cost = mean_squared_error(x, gx)\n",
    "        psnr_i = peak_signal_to_noise_ratio(x, gx)\n",
    "        psnr_all += psnr_i\n",
    "        summaries.append(self.prefix + '_recon_cost' + str(i) + ': ' + str(recon_cost.data))\n",
    "        summaries.append(self.prefix + '_psnr' + str(i) + ': ' + str(psnr_i.data))\n",
    "        loss += recon_cost\n",
    "\n",
    "    for i, state, gen_state in zip(range(len(gen_states)), states[num_frame_before_prediction:], gen_states[num_frame_before_prediction - \n",
    "        state = variable.Variable(state)\n",
    "        state_cost = mean_squared_error(state, gen_state) * 1e-4\n",
    "        summaries.append(self.prefix + '_state_cost' + str(i) + ': ' + str(state_cost.data))\n",
    "        loss += state_cost\n",
    "\n",
    "    summaries.append(self.prefix + '_psnr_all: ' + str(psnr_all.data))\n",
    "    self.psnr_all = psnr_all\n",
    "    self.loss = loss = loss / np.float32(len(images) - num_frame_before_prediction)\n",
    "    summaries.append(self.prefix + '_loss: ' + str(loss.data))\n",
    "\n",
    "    return self.loss, self.psnr_all, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network consist of seven LSTM cells and seven convolutions/deconvolutions that are usually similar between each variation of the model. Below is an example of a convolution, hidden LSTM cell and deconvolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "enc0 = slim.layers.conv2d(prev_image, 32, [5, 5], stride=2, scope='scale1_conv1', normalizer_fn=tf_layers.layer_norm, \n",
    "                          normalizer_params={'scope': 'layer_norm1'})\n",
    "\n",
    "hidden1, lstm_state1 = lstm_func(enc0, lstm_state1, lstm_size[0], scope='state1')\n",
    "hidden1 = tf_layers.layer_norm(hidden1, scope='layer_norm2')\n",
    "\n",
    "# ...\n",
    "\n",
    "enc4 = slim.layers.conv2d_transpose(hidden5, hidden5.get_shape()[3], 3, stride=2, scope='convt1')\n",
    "\n",
    "# Chainer\n",
    "enc0 = L.Convolution2D(in_channels=3, out_channels=32, ksize=(5, 5), stride=2, pad=5/2)(prev_image)\n",
    "# TensorFlow code use layer_normalization for normalize on the output convolution\n",
    "enc0 = layer_normalization_conv_2d(enc0)\n",
    "\n",
    "hidden1, lstm_state1 = self.stateless_lstm(inputs=enc0, state=lstm_state1, num_channels=32)\n",
    "hidden1 = layer_normalization_conv_2d(hidden1)\n",
    "\n",
    "# ...\n",
    "\n",
    "enc4 = L.Deconvolution2D(in_channels=hidden5.shape[1], out_channels=hidden5.shape[1], ksize=(3,3), \n",
    "                         stride=2, outsize=(hidden5.shape[2]*2, hidden5.shape[3]*2), pad=3/2)(hidden5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally each transformations outputed by the different type of models, are used to create a list of masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "#...\n",
    "masks = slim.layers.conv2d_transpose(\n",
    "    enc6, num_masks + 1, 1, stride=1, scope='convt7')\n",
    "masks = tf.reshape(masks, [-1, num_masks + 1])\n",
    "masks = tf.nn.softmax(masks)\n",
    "masks = tf.reshape(masks, [int(batch_size), int(img_height), int(img_width), num_masks + 1])\n",
    "mask_list = tf.split(axis=3, num_or_size_splits=num_masks + 1, value=masks)\n",
    "output = mask_list[0] * prev_image\n",
    "for layer, mask in zip(transformed, mask_list[1:]):\n",
    "  output += layer * mask\n",
    "gen_images.append(output)\n",
    "\n",
    "# Chainer\n",
    "#...\n",
    "masks = L.Deconvolution2D(in_channels=enc6.shape[1], out_channels=num_masks+1, ksize=(1,1), stride=1)(enc6)\n",
    "masks = F.reshape(masks, (-1, num_masks + 1))\n",
    "masks = F.softmax(masks)\n",
    "masks = F.reshape(masks, (int(batch_size), num_masks+1, int(img_height), int(img_width))) # Previously num_mask at the end, but  on axis=1? ok!\n",
    "mask_list = F.split_axis(masks, indices_or_sections=num_masks+1, axis=1) # Previously axis=3 but our channels are on axis=1 ?\n",
    "output = F.scale(prev_image, mask_list[0], axis=0)\n",
    "for layer, mask in zip(transformed, mask_list[1:]):\n",
    "    output += F.scale(layer, mask, axis=0)\n",
    "gen_images.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StatelessCDNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model outputs multiple normalized convolution kernels to apply to the previous image to compute new pixels value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "#...\n",
    "\n",
    "def cdna_transformation(prev_image, cdna_input, num_masks, color_channels):\n",
    "  batch_size = int(cdna_input.get_shape()[0])\n",
    "\n",
    "  # Predict kernels using linear function of last hidden layer.\n",
    "  cdna_kerns = slim.layers.fully_connected(\n",
    "      cdna_input,\n",
    "      DNA_KERN_SIZE * DNA_KERN_SIZE * num_masks,\n",
    "      scope='cdna_params',\n",
    "      activation_fn=None)\n",
    "\n",
    "  # Reshape and normalize.\n",
    "  cdna_kerns = tf.reshape(\n",
    "      cdna_kerns, [batch_size, DNA_KERN_SIZE, DNA_KERN_SIZE, 1, num_masks])\n",
    "  cdna_kerns = tf.nn.relu(cdna_kerns - RELU_SHIFT) + RELU_SHIFT\n",
    "  norm_factor = tf.reduce_sum(cdna_kerns, [1, 2, 3], keep_dims=True)\n",
    "  cdna_kerns /= norm_factor\n",
    "\n",
    "  cdna_kerns = tf.tile(cdna_kerns, [1, 1, 1, color_channels, 1])\n",
    "  cdna_kerns = tf.split(axis=0, num_or_size_splits=batch_size, value=cdna_kerns)\n",
    "  prev_images = tf.split(axis=0, num_or_size_splits=batch_size, value=prev_image)\n",
    "\n",
    "  # Transform image.\n",
    "  transformed = []\n",
    "  for kernel, preimg in zip(cdna_kerns, prev_images):\n",
    "    kernel = tf.squeeze(kernel)\n",
    "    if len(kernel.get_shape()) == 3:\n",
    "      kernel = tf.expand_dims(kernel, -1)\n",
    "    conv = tf.nn.depthwise_conv2d(preimg, kernel, [1, 1, 1, 1], 'SAME')\n",
    "    transformed.append(conv)\n",
    "  transformed = tf.concat(axis=0, values=transformed)\n",
    "  transformed = tf.split(axis=3, num_or_size_splits=num_masks, value=transformed)\n",
    "\n",
    "\n",
    "# Chainer\n",
    "#... \n",
    "cdna_input = F.reshape(hidden5, (int(batch_size), -1))\n",
    "cdna_kerns = L.Linear(in_size=None, out_size=5*5*num_masks)(cdna_input)\n",
    "\n",
    "# Reshape and normalize\n",
    "#cdna_kerns = np.reshape(cdna_kerns, (batch_size, 5, 5, 1, num_masks))\n",
    "cdna_kerns = F.reshape(cdna_kerns, (batch_size, 1, 5, 5, num_masks))\n",
    "cdna_kerns = F.relu(cdna_kerns - 1e-12) + 1e-12\n",
    "norm_factor = sum.sum(cdna_kerns, (1, 2, 3), keepdims=True)\n",
    "\n",
    "# The norm factor is broadcasted to match the shape difference\n",
    "axis_reshape = 0\n",
    "norm_factor_new_shape = tuple([1] * axis_reshape + list(norm_factor.shape) +\n",
    "                               [1] * (len(cdna_kerns.shape) - axis_reshape - len(norm_factor.shape)))\n",
    "norm_factor = F.reshape(norm_factor, norm_factor_new_shape)\n",
    "norm_factor_broadcasted = F.broadcast_to(norm_factor, cdna_kerns.shape)\n",
    "cdna_kerns = cdna_kerns / norm_factor_broadcasted\n",
    "\n",
    "cdna_kerns = F.tile(cdna_kerns, (1,3,1,1,1))\n",
    "cdna_kerns = F.split_axis(cdna_kerns, indices_or_sections=batch_size, axis=0)\n",
    "prev_images = F.split_axis(prev_image, indices_or_sections=batch_size, axis=0)\n",
    "\n",
    "# Transform image\n",
    "tmp_transformed = []\n",
    "for kernel, preimg in zip(cdna_kerns, prev_images):\n",
    "    kernel = F.squeeze(kernel)\n",
    "    if len(kernel.shape) == 3:\n",
    "        kernel = kernel[..., np.keepdims]\n",
    "    conv = L.DepthwiseConvolution2D(in_channels=preimg.shape[1], channel_multiplier=kernel.shape[3], ksize=(kernel.shape[1], , stride=1, pad=kernel.shape[1]/2)(preimg)\n",
    "    tmp_transformed.append(conv)\n",
    "tmp_transformed = F.concat(tmp_transformed, axis=0)\n",
    "tmp_transformed = F.split_axis(tmp_transformed, indices_or_sections=num_masks, axis=1) # Previously axis=3 but our channels are on \n",
    "transformed = transformed + list(tmp_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StatelessDNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model outputs a distribution over locations in the previous frame for each pixel in the new frame. \n",
    "The predicted value becomes the expectation under the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "def dna_transformation(prev_image, dna_input):\n",
    "  # Construct translated images.\n",
    "  prev_image_pad = tf.pad(prev_image, [[0, 0], [2, 2], [2, 2], [0, 0]])\n",
    "  image_height = int(prev_image.get_shape()[1])\n",
    "  image_width = int(prev_image.get_shape()[2])\n",
    "\n",
    "  inputs = []\n",
    "  for xkern in range(DNA_KERN_SIZE):\n",
    "    for ykern in range(DNA_KERN_SIZE):\n",
    "      tmp = tf.slice(prev_image_pad, [0, xkern, ykern, 0], [-1, image_height, image_width, -1])\n",
    "      tmp = tf.expand_dims(tmp, [3])\n",
    "      inputs.append(tmp)\n",
    "  inputs = tf.concat(axis=3, values=inputs)\n",
    "\n",
    "  # Normalize channels to 1.\n",
    "  kernel = tf.nn.relu(dna_input - RELU_SHIFT) + RELU_SHIFT\n",
    "  kernel_sum = tf.reduce_sum(kernel, [3], keep_dims=True)\n",
    "  kernel = kernel / kernel_sum\n",
    "  kernel = tf.expand_dims(kernel, [4])\n",
    "  kernel = tf.reduce_sum(kernel * inputs, [3], keep_dims=False)\n",
    "  return kernel\n",
    "\n",
    "# Chainer\n",
    "# ...\n",
    "prev_image_pad = F.pad(prev_image, pad_width=[[0,0], [0,0], [2,2], [2,2]], mode='constant', constant_values=0)\n",
    "kernel_inputs = []\n",
    "for xkern in range(5):\n",
    "    for ykern in range(5):\n",
    "        #tmp = F.get_item(prev_image_pad, [prev_image_pad.shape[0], prev_image_pad.shape[0], xkern:img_height, ykern:img_width])\n",
    "        tmp = F.get_item(prev_image_pad, list([slice(0,prev_image_pad.shape[0]), slice(0,prev_image_pad.shape[1]), slice(), slice(ykern,img_width)]))\n",
    "        # ** Added this operation to make sure the size was still the original one!\n",
    "        tmp = F.pad(tmp, [[0,0], [0,0], [0, xkern], [0, ykern]], mode='constant', constant_values=0)\n",
    "        tmp = F.expand_dims(tmp, axis=1) # Previously axis=3 but our channel is on axis=1 ? ok!\n",
    "        kernel_inputs.append(tmp.data)\n",
    "kernel_inputs = F.concat(kernel_inputs, axis=1) # Previously axis=3 but our channel us on axis=1 ? ok!\n",
    "\n",
    "# Normalize channels to 1\n",
    "kernel_normalized = F.relu(enc7 - 1e-12) + 1e+12\n",
    "kernel_normalized_sum = F.sum(kernel_normalized, axis=1, keepdims=True) # Previously axis=3 but our channel are on axis 1 ? ok!\n",
    "kernel_normalized = broadcasted_division(kernel_normalized, kernel_normalized_sum)\n",
    "kernel_normalized = F.expand_dims(kernel_normalized, axis=2)\n",
    "kernel_normalized = F.scale(kernel_inputs, kernel_normalized, axis=0)\n",
    "kernel_normalized = F.sum(kernel_normalized, axis=1, keepdims=False)\n",
    "transformed = [kernel_normalized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StatelessSTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model outputs the parameters of multiple affine transformations to apply to the previous image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorFloat\n",
    "def stp_transformation(prev_image, stp_input, num_masks):\n",
    "  # Only import spatial transformer if needed.\n",
    "  from spatial_transformer import transformer\n",
    "\n",
    "  identity_params = tf.convert_to_tensor(\n",
    "      np.array([1.0, 0.0, 0.0, 0.0, 1.0, 0.0], np.float32))\n",
    "  transformed = []\n",
    "  width, height = prev_image.get_shape()[1:3]\n",
    "  for i in range(num_masks - 1):\n",
    "    params = slim.layers.fully_connected(\n",
    "        stp_input, 6, scope='stp_params' + str(i),\n",
    "        activation_fn=None) + identity_params\n",
    "    trans = transformer(prev_image, params)\n",
    "    transformed.append(trans)\n",
    "\n",
    "  return transformed\n",
    "\n",
    "# Chainer\n",
    "#...\n",
    "stp_input0 = F.reshape(hidden5, (int(batch_size), -1))\n",
    "stp_input1 = L.Linear(in_size=None, out_size=100)(stp_input0)\n",
    "identity_params = np.array([[1.0, 0.0, 0.0, 0.0, 1.0, 0.0]], dtype=np.float32)\n",
    "identity_params = np.repeat(identity_params, int(batch_size), axis=0)\n",
    "identity_params = variable.Variable(identity_params)\n",
    "\n",
    "stp_transformations = []\n",
    "for i in range(num_masks-1):\n",
    "    params = L.Linear(in_size=None, out_size=6)(stp_input1) + identity_params\n",
    "    params = F.reshape(params, (int(params.shape[0]), 2, 3))\n",
    "    grid = F.spatial_transformer_grid(params, (prev_image.shape[2], prev_image.shape[3]))\n",
    "    trans = F.spatial_transformer_sampler(prev_image, grid)\n",
    "    stp_transformations.append(trans)\n",
    "\n",
    "transformed += stp_transformations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
